* Add bigmemory dataset

* Add translation dataset

* Uniform grid for massive dimensionality data (sparse vector from boost?)
    having done that we can make splitting more efficient
    because currently it has to look in 3^d cubes, and 
    for d>10 it is equivalent to looking at all points 
    I would use external library

* Refactor to bagging datasets, improve rejection sampling

* Add example should pause the algorithm - no, it shouldn't,
however there should be a mutex when getting sample

* TODO: visual test - common format visualisation igraph compatible

* Error to GNGAlgorithm

* Graph refactoring

* Refactor m_grow_rate -> m_shrink_rate

* Exceptions


* Check delete in GNGNode